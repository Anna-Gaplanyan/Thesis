{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2532d265",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c43ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340d09f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "armenian_prepositions = [\"և\",\"եւ\" \"ու\", \"ապա\", \"բայց\", \"իսկ\", \"սակայն\", \"այլ\", \"այլև\", \"կամ\", \"նաև\", \"անգամ\", \"մանավանդ\", \"այսինքն\",\"ուրեմն\", \"բայցև\", \"բայց և այնպես\", \"այն է\", \"ինչպես\", \"մինչև իսկ\"\n",
    ", \"կամ թե\", \"և կամ\", \"ապա թե ոչ\", \"և՛\",\"ո՛չ\", \"թե՛\", \"կա՛մ\", \"եթե\", \"թե\", \"որ\", \"թե որ\", \"որպեսզի\", \"որովհետև\", \"թեև\",\n",
    " \"թեկուզ\", \"թեպետ\", \"թեպետև\", \"մինչ\", \"մինչև\", \"մինչև որ\", \"մինչդեռ\", \"քանի\", \"քանի որ\", \"թեկուզև\", \"հենց\", \"հենց որ\",\n",
    " \"եթե ոչ\", \"քանի դեռ\", \"չնայած\", \"չնայած որ\", \"նախքան\", \"թե չէ\", \"էլ\", \"քան\", \"ուրեմն\", \"քանզի\", \"այնքան\", \"ուստի\",\n",
    " \"քան թե\", \"այնուամենայնիվ\", \"այսուամենայնիվ\", \"այսուհանդերձ\", \"այնուհանդերձ\", \"համենայն դեպս\", \"չէ որ\", \"ասես\", \"կարծես\", \"թեպետ\"]\n",
    "len(armenian_prepositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3975eccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "armenian_relatives = [\"անշուշտ\", \"գուցե\", \"նամանավանդ\", \"հավանաբար\", \"այո՛\", \"անկասկած\", \"անպայման\", \"անպատճառ\", \"անտարակույս\", \"առանց այլևայլության\",\n",
    " \"արդարև\", \"բնականաբար\", \"իհարկե\", \"ինչ ասել կուզի\", \"ինչ խոսք\", \"ինչպե՞ս չէ\", \"ինչու՞ չէ\", \"իսկապես\", \"իրավ\", \"իրավամբ\", \"իրոք\", \"հիրավի\",\n",
    " \"հարկավ\", \"ո՛չ\", \"չէ՛\", \"բնավ\", \"ոչ մի դեպքում\", \"ոչ մի կերպ\", \"ասես\",\"արդյոք\", \"գուցե\", \"դժվար թե\", \"երևի\", \"ըստ երևույթի\",\n",
    " \"թերևս\", \"իբր\", \"իբրև\", \"ինչպես երևում է\",\"կարծեմ\", \"կարծես\", \"կարող էպատահել\", \"հազիվ թե\", \"հավանաբար\", \"հնարավոր է\",\n",
    " \"միգուցե\", \"մի՞թե\", \"չլինի՞\", \"ախր\", \"ափսո՛ս\", \"բարեբախտաբար\", \"դժբախտաբար\", \"ի՞նչ արած\",\n",
    " \"չէ՞ որ\", \"տարաբախտաբար\", \"ցավոք\", \"ցավոք սրտի\", \"անգամ\", \"ևեթ\", \"իսկ\"\n",
    ", \"իսկ և իսկ\", \"հենց\", \"մանավանդ\", \"մինչև անգամ\", \"մինչև իսկ\",\n",
    " \"նամանավանդ\", \"նույնիսկ\",\"գեթ\", \"գոնե\", \"թեկուզ\", \"լոկ\", \"միայն\", \"միմիայն\", \"սոսկ\", \"արի\", \"դե\", \"եկ\", \"եկեք\", \"թող\", \"ապա\",\n",
    " \"մի\", \"երանի\", \"երնեկ\", \"ուր է թե\", \"այնուամենայնիվ\", \"այնուհանդերձ\",\n",
    "\"այսուամենայնիվ\",\"այսուհանդերձ\", \"ինչորէ\", \"ինչ էլ լինի\", \"ինչ ուզում է լինի\", \"ինչ գնով էլ լինի\", \"ինչևէ\", \"ինչևիցե\", \"համենայնդեպս\",\n",
    " \"ճիշտ է\", \"մեկ է\", \"միևնույն է\", \"ընդհակառակն\", \"ի դեպ\", \"իմիջիայլոց\",\"այսպիսով\", \"մի խոսքով\", \"ասենք\", \"արդ\", \"այսպես ասած\", \"ավեի ճիշտ\",\n",
    " \"ահա\", \"ահավասիկ\", \"ա՛յ\", \"ասենք թե\", \"հասկանալի է\", \"հաճելի է\", \"ցանկալի է\", \"կհավատա՞ս\",\n",
    " \"ասում են\", \"ճիշտ որ\", \"հաստատ\", \"էլ դու սուս\", \"հարց չկա\", \"բայց և այնպես\", \"չի կարող պատահել\", \"դե\", \"ահա\",\n",
    " \"իսկապես\", \"մինչև իսկ\", \"իրոք որ\", \"ինչ է թե\"\n",
    " ]\n",
    "len(armenian_relatives) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5373381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "armenian_kap = [\"առ\", \"առանց\", \"դեպի\", \"զերդ\", \"ընդ\", \"ըստ\", \"ի\", \"ի դեմս\", \"ի հաշիվ\", \"ի հեճուկս\", \"ի փառս\", \"իբր\", \"իբրև\", \"որպես\", \"հանձին\", \"հանձինս\", \"հանուն\",\n",
    " \"հօգուտ\", \"մինչ\", \"մինչև\", \"նախքան\", \"փոխանակ\", \"քան\", \"անց\", \"առթիվ\", \"առջև\", \"դեմ\", \"դիմաց\", \"զատ\", \"համար\", \"հանդերձ\", \"հետ\", \"մասին\", \"մեջ\",\n",
    " \"միջև\", \"ներքո\", \"նման\", \"շուրջ\", \"շուրջը\", \"պես\", \"վերաբերյալ\", \"վերաբերմամբ\", \"վրա\", \"տակ\", \"փոխարեն\", \"օգտին\", \"օրոք\", \"բացի\", \"ընդդեմ\", \"հանդեպ\",\n",
    " \"շնորհիվ\", \"առաջ\", \"անկախ\", \"անցած\", \"անունից\", \"անունով\", \"առթիվ\", \"բացառությամբ\", \"գծով\", \"դեպքում\", \"դուրս\", \"երեսից\", \"զուգընթաց\", \"ընթացքում\",\n",
    " \"ժամանակ\", \"կից\", \"կողմից\", \"հակառակ\", \"համաձայն\", \"համապատասխան\", \"հեռու\",\n",
    "\"հետո\", \"ձեռքով\", \"միջոցով\", \"մոտ\", \"ներս\", \"ներքև\", \"նպատակով\",\n",
    " \"չանցած\", \"պատճառով\", \"սկսած\", \"վար\", \"վեր\", \"տարբեր\", \"տեղ\", \"ուղղությամբ\",\n",
    "\"ցած\", \"դեպի\", \"մինչ\", \"նախքան\", \"չանցած\", \"չհաշված\", \"դեմ\",\n",
    " \"դիմաց\", \"առթիվ\", \"առջև\", \"երեսից\", \"համար\", \"կողմից\", \"մասին\", \"մեջ\", \"նկատմամբ\", \"նպատակով\", \"պես\", \"պատճառով\", \"տեղ\", \"վերաբերյալ\",\n",
    " \"հանդերձ\", \"մեկտեղ\", \"մեկ\", \"անց\", \"չափ\", \"ըննդեմ\", \"նախքան\", \"ի վեր\", \"ի պատիվ\", \"անկախ\", \"անցած\", \"անունից\", \"անունով\", \"առաջ\", \"առթիվ\", \"բացառյալ\",\n",
    " \"բացառությամբ\", \"գծով\", \"դեպքում\", \"դուրս\", \"երեսից\", \"զուգընթաց\", \"ընթացքում\", \"ժամանակ\", \"ի վար\", \"կից\", \"կողմ\", \"կողմից\", \"հակառակ\",\n",
    " \"համաձայն\", \"համապատասխան\", \"համեմատ\", \"հանդիման\", \"հավասար\", \"հեռու\", \"հետո\", \"հետևանքով\", \"հիման վրա\", \"ձեռքով\", \"միջոցին\", \"միջոցով\", \"մոտ\",\n",
    " \"ներս\", \"ներքև\", \"նկատմամբ\", \"նպատակով\", \"չանցած\", \"չափ\", \"չնայած\", \"պատճառով\", \"սկսած\", \"վար\", \"վեր\", \"վերև\", \"տարբեր\", \"տեղ\", \"ուղղությամբ\", \"ցած\",\n",
    " \"բացառյալ\", \"բացառությամբ\", \"նայած\", \"չնայած\", \"սկսած\", \"անկախ\", \"անցած\", \"հակառակ\", \"համաձայն\", \"համապատասխան\", \"հեռու\", \"ներառյալ\",\n",
    " \"չանցած\", \"չհաշված\"]\n",
    "len(armenian_kap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66a6603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "armenian_pronoun = [\"ես\", \"ինքս\", \"դու\", \"ինքդ\", \"նա\", \"ինքը\", \"մենք\", \"ինքներս\", \"դուք\", \"ինքներդ\", \"նրանք\", \"իրենք\", \"իմ\", \"մեր\", \"ինձ\", \"մեզ\", \"ինձնից\",\n",
    " \"ինձանից\", \"մեզնից\", \"մեզանից\", \"ինձնով\", \"ինձանով\", \"մեզնով\", \"մեզանով\", \"ինձնում\", \"ինձանում\" \"մեզնում\", \"մեզանում\", \"դուք\", \"քո\",\n",
    " \"ձեր\", \"քեզ\", \"ձեզ\", \"քեզնից\", \"քեզանից\", \"ձեզնից\", \"ձեզանից\", \"քեզնով\", \"քեզանով\", \"ձեզնով\", \"ձեզանով\", \"քեզնում\", \"քեզանում\", \"ձեզնում\",\n",
    " \"ձեզանում\", \"նա\", \"նրանք\", \"ինքը\", \"իրենք\", \"նրա\", \"նրանց\", \"իր\", \"իրենց\", \"նրան\", \"նրանց\", \"իրեն\", \"իրենց\", \"նրանից\", \"նրանցից\", \"իրենից\",\n",
    " \"իրենցից\", \"նրանով\", \"նրանցով\", \"իրենով\", \"իրենցով\", \"նրանում\", \"նրանցում\", \"իրենում\", \"իրենցում\", \"իրար\", \"միմյանց\", \"մեկմեկու\", \"մեկմեկի\",\n",
    " \"իրարից\", \"միմյանցից\", \"մեկմեկուց\", \"մեկմեկից\", \"իրարով\", \"միմյանցով\",\"մեկմեկով\", \"սա\", \"դա\", \"նա\", \"այս\", \"այդ\", \"այն\", \"սույն\", \"նույն\", \"միևնույն\", \"մյուս\", \"այսպես\", \"այդպես\", \"այնպես\", \"նույնպես\", \"այսքան\"\n",
    ", \"այդքան\", \"այնքան\", \"նույնքան\", \"այստեղ\", \"այնտեղ\", \"այդտեղ\", \"նույնտեղ\",\"այսչափ\", \"այնչափ\", \"այդչափ\", \"նույնչափ\", \"այսպիսի\", \"այնպիսի\", \"այդպիսի\", \"նույնպիսի\", \"սա\", \"սրանք\", \"դա\", \"դրանք\", \"սրա\", \"սրանց\", \"դրա\",\n",
    " \"դրանց\", \"սրան\", \"սրանք\", \"դրան\", \"դրանք\", \"սրանից\", \"սրանցից\", \"դրանից\", \"դրանցից\", \"սրանով\", \"սրանցով\", \"դրանով\", \"դրանցով\", \"սրանում\",\n",
    " \"սրանցում\", \"դրանում\", \"դրանցում\", \"ո՞վ\", \"ի՞նչ\", \"ո՞ր\", \"ու՞ր\", \"ե՞րբ\", \"ինչու՞\", \"ինչպիսի՞\", \"ինչքա՞ն\", \"ինչպե՞ս\", \"ինչչա՞փ\", \"քա՞նի\",\n",
    " \"ո՞րքան\", \"քանի՞երորդ\", \"ո՞րերորդ\", \"ո՞րտեղ\", \"ո՞րչափ\", \"ի՞նչը\", \"ինչե՞ր\", \"ինչե՞րը\", \"ո՞րը\", \"ու՞մ\", \"ինչի՞\", \"ինչերի՞\", \"որի՞\", \"ինչի՞ն\",\n",
    " \"ինչերի՞ն\", \"որի՞ն\", \"ինչե՞րը\", \"ո՞րը\", \"ումի՞ց\", \"ինչի՞ց\", \"ինչերի՞ց\",\"որի՞ց\", \"ումո՞վ\", \"ինչո՞վ\", \"ինչերո՞վ\", \"որո՞վ\", \"ինչու՞մ\", \"ինչերու՞մ\",\n",
    " \"որու՞մ\", \"որո՞նք\", \"որո՞նց\", \"որոնցի՞ց\", \"որոնցո՞վ\", \"որոնցու՞մ\", \"որտե՞ղ\", \"ե՞րբ\", \"ամենքը\", \"ամեն մի\", \"ամեն մեկը\", \"ամեն ինչ\", \"ամեն ոք\",\n",
    " \"յուրաքանչյուր\", \"յուրաքանչյուր ոք\", \"յուրաքանչյուր մեկը\", \"բոլորը\", \"ողջ\", \"ամբողջ\", \"համայն\", \"ամենայն\", \"մեկը\", \"մեկնումեկը\", \"ինչ-որ\", \"ինչ-ինչ\",\n",
    " \"ուրիշ\", \"որոշ\", \"մի\", \"ոմն\", \"մի քանի\", \"մի երկու\", \"այնինչ\", \"այսինչ\", \"այլ\", \"ովևէ\", \"ովևիցե\", \"որևէ\", \"որևիցե\", \"երբևէ\", \"երբևիցե\", \"ուրևէ\",\n",
    " \"ուրևիցե\", \"ոմանք\", \"ոմանց\", \"ոմանցից\", \"մի քանիս\", \"մի քանիսը\", \"մի քանիսի\", \"մի քանիսից\", \"այլք\", \"այլոց\", \"ոչ ոք\", \"ոչ մի\", \"ոչ մեկը\", \"ոչիինչ\",\n",
    " \"ոչ ոքի\"]\n",
    "len(armenian_pronoun) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec4f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "armenian_auxiliary_verb = [\"եմ\", \"ես\", \"է\", \"ենք\", \"եք\", \"են\", \"էի\", \"էիր\", \"էր\", \"էինք\", \"էիք\", \"էին\", \"չեմ\", \"չես\", \"չէ\", \"չի\", \"չենք\", \"չեք\", \n",
    "   \"չեն\", \"չէի\", \"չէիր\", \"չէր\", \"չէինք\", \"չէիք\", \"չէին\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6cf7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "armenian_stopwords = armenian_prepositions + armenian_relatives +armenian_kap + armenian_pronoun + armenian_auxiliary_verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb28ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "armenian_stopwords = list(set(armenian_stopwords))\n",
    "len(armenian_stopwords)\n",
    "armenian_punctuations = [\":\", \".\", \",\", \"`\", \"՛\", \"՞\", \"՜\", \"«\", \"»\", \"-\", \"–\", \"…\", \"․․․․\", \"(\", \")\", \"[\", \"]\", \"՚\", \"֊\"]\n",
    "armenian_stopwords_punct = armenian_stopwords + armenian_punctuations\n",
    "data_training_english = pd.read_csv(\"drive/My Drive/really_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4cf36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i in range(data_training_english.text.shape[0]):\n",
    " text1 = data_training_english.title[i]\n",
    " text2 = data_training_english.text[i]\n",
    " text = str(text1) +\"\"+ str(text2)\n",
    " texts.append(text)\n",
    "data_training_english[\"full_text\"] = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511bae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicates:\", data_training_english.duplicated().sum())\n",
    "print(\"Missing values:\", data_training_english.isna().sum().sum())\n",
    "print(\"Single valued columns:\", data_training_english.columns[data_training_english.nunique()==1])\n",
    "data_training_english = data_training_english.drop_duplicates()\n",
    "data_training_english = data_training_english.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de28beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stpwrd = nltk.corpus.stopwords.words('english')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af91e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training_english[\"full_text\"] = data_training_english[\"full_text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stpwrd)]))\n",
    "len(data_training_english[\"full_text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training_english.to_csv(\"no_stopwords.csv\", index=False)\n",
    "data_training_english.to_excel(\"no_stopwords.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c49d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_in_Armenian = pd.read_csv(\"drive/My Drive/really_final_armenian_utf.csv\")\n",
    "\n",
    "textsarm = []\n",
    "\n",
    "\n",
    "for i in range(articles_in_Armenian.text.shape[0]):\n",
    "    text1 = articles_in_Armenian.title[i]\n",
    "    text2 = articles_in_Armenian.text[i]\n",
    "    text = str(text1) +\"\"+ str(text2)\n",
    "    textsarm.append(text)\n",
    "articles_in_Armenian[\"full_text\"] = textsarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc791c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicates:\", articles_in_Armenian.duplicated().sum())       \n",
    "print(\"Missing values:\", articles_in_Armenian.isna().sum().sum())   \n",
    "print(\"Single valued columns:\", articles_in_Armenian.columns[articles_in_Armenian.nunique()==1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48698097",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_in_Armenian = articles_in_Armenian.drop_duplicates()\n",
    "articles_in_Armenian = articles_in_Armenian.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce277107",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles_in_Armenian[\"full_text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289001e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_in_Armenian[\"full_text\"] = articles_in_Armenian[\"full_text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (armenian_stopwords_punct)]))\n",
    "len(articles_in_Armenian[\"full_text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56672d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_in_Armenian.to_csv(\"armenian_no_stopwords.csv\", index=False)\n",
    "articles_in_Armenian.to_excel(\"armenian_no_stopwords.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd935e24",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53411d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, concatenate, Dropout\n",
    "from keras.models import Model\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1931889",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"drive/My Drive/train_data.csv\")\n",
    "data_train[\"type\"] =np.where(data_train[\"type\"]=='Fake',0,1)\n",
    "texts = []\n",
    "labels = []\n",
    "for i in range(data_train.text.shape[0]):\n",
    " text1 = data_train.title[i]\n",
    " text2 = data_train.text[i]\n",
    " text = str(text1) +\"\"+ str(text2)\n",
    " texts.append(text)\n",
    " labels.append(data_train.type[i])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb429908",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels),num_classes = 2)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "# Train test validation Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "x_train, x_test, y_train, y_test = train_test_split( data, labels, test_size=0.20, random_state=42)\n",
    "x_test, x_val, y_test, y_val = train_test_split( x_test, y_test, test_size=0.50, random_state=42)\n",
    "print('Size of train, validation, test:', len(y_train), len(y_val), len(y_test))\n",
    "print('real & fake news in train,valt,test:')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))\n",
    "print(y_test.sum(axis=0))\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c4a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = \"drive/My Drive/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    " values = line.split()\n",
    " word = values[0]\n",
    " coefs = np.asarray(values[1:], dtype='float32')\n",
    " embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Total %s word vectors in Glove.' % len(embeddings_index))\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    " embedding_vector = embeddings_index.get(word)\n",
    " if embedding_vector is not None:\n",
    " embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    " EMBEDDING_DIM,\n",
    "weights=[embedding_matrix],\n",
    " input_length=MAX_SEQUENCE_LENGTH)\n",
    "embedding_vecor_length = 32\n",
    "modell = Sequential()\n",
    "modell.add(embedding_layer)\n",
    "modell.add(Dropout(0.2))\n",
    "modell.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'))\n",
    "modell.add(MaxPooling1D(pool_size=2))\n",
    "modell.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "modell.add(MaxPooling1D(pool_size=2))\n",
    "modell.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "modell.add(BatchNormalization())\n",
    "modell.add(Dense(256, activation='relu'))\n",
    "modell.add(Dense(128, activation='relu'))\n",
    "modell.add(Dense(64, activation='relu'))\n",
    "modell.add(Dense(2, activation='softmax'))\n",
    "modell.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(modell.summary())\n",
    "modell.fit(x_train, y_train, epochs=35, batch_size=64)\n",
    "modell.save('lstm_with_35_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d8372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "test_preds = modell.predict(x_test)\n",
    "test_preds = np.round(test_preds)\n",
    "correct_predictions = float(sum(test_preds == y_test)[0])\n",
    "print(\"Correct predictions:\", correct_predictions)\n",
    "print(\"Total number of test examples:\", len(y_test))\n",
    "print(\"Accuracy of model1: \", correct_predictions/float(len(y_test)))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "x_pred = modell.predict(x_test)\n",
    "x_pred = np.round(x_pred)\n",
    "x_pred = x_pred.argmax(1)\n",
    "y_test_s = y_test.argmax(1)\n",
    "cm = confusion_matrix(y_test_s, x_pred)\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(cm,annot=True,annot_kws={\"size\": 16},fmt='1f')# font size\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test_s, x_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728bc585",
   "metadata": {},
   "source": [
    "## Labeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b57193",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = keras.models.load_model(\"drive/My Drive/lstm_with_35_epochs.h5\")\n",
    "article_data = pd.read_csv(\"drive/My Drive/no_stopwords.csv\")\n",
    "texts = []\n",
    "for i in range(article_data.text.shape[0]):\n",
    " text1 = article_data.title[i]\n",
    " text2 = article_data.text[i]\n",
    " text = str(text1) +\"\"+ str(text2)\n",
    " texts.append(text)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b50233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "predictions = model_lstm.predict(data)\n",
    "x_pred = np.round(predictions)\n",
    "x_pred = x_pred.argmax(1)\n",
    "article_data[\"type\"] = x_pred\n",
    "new_data = pd.read_csv(\"drive/My Drive/armenian_no_stopwords_utf.csv\")\n",
    "df3 = pd.merge(article_data, new_data, on='links')\n",
    "df3.drop([\"title_x\", \"text_x\", \"full_text_x\"], axis = 1, inplace = True)\n",
    "article_data.to_excel(\"classes_English.xlsx\", index = False)\n",
    "df3.to_excel(\"classes_Armenian.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964eb9c4",
   "metadata": {},
   "source": [
    "## Modeling with Armenian dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, concatenate, Dropout\n",
    "from keras.models import Model\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7172c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_in_Armenian = pd.read_excel(\"drive/My Drive/classes_Armenian.xlsx\")\n",
    "articles_in_Armenian.rename( columns={\"title_y\": \"title\" } ,inplace=True)\n",
    "articles_in_Armenian.rename( columns={\"text_y\": \"text\" } ,inplace=True)\n",
    "texts = articles_in_Armenian[\"full_text_y\"]\n",
    "labels = []\n",
    "for i in range(articles_in_Armenian.text.shape[0]):\n",
    " labels.append(articles_in_Armenian[\"type\"][i])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels),num_classes = 2)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "x_train, x_test, y_train, y_test = train_test_split( data, labels, test_size=0.2, random_state=42, stratify = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbcadde",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = \"drive/My Drive/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove200ws80.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    " values = line.split()\n",
    " #print(values[1:])\n",
    " word = values[0]\n",
    " coefs = np.asarray(values[1:], dtype='float32')\n",
    " embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Total %s word vectors in Glove.' % len(embeddings_index))\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    " embedding_vector = embeddings_index.get(word)\n",
    " if embedding_vector is not None:\n",
    "\n",
    " embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    " EMBEDDING_DIM,\n",
    "weights=[embedding_matrix],\n",
    "input_length=MAX_SEQUENCE_LENGTH)\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers import BatchNormalization\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vecor_length = 32\n",
    "modell = Sequential()\n",
    "modell.add(embedding_layer)\n",
    "modell.add(Dropout(0.2))\n",
    "modell.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'))\n",
    "modell.add(MaxPooling1D(pool_size=2))\n",
    "modell.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "modell.add(MaxPooling1D(pool_size=2))\n",
    "modell.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
    "modell.add(BatchNormalization())\n",
    "modell.add(Dense(256, activation='relu'))\n",
    "modell.add(Dense(256, activation='relu'))\n",
    "modell.add(Dense(256, activation='relu'))\n",
    "modell.add(Dense(256, activation='relu'))\n",
    "modell.add(Dense(2, activation='softmax'))\n",
    "modell.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(modell.summary())\n",
    "history = modell.fit(x_train, y_train, validation_split=0.2, epochs= 30, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db098691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "test_preds = modell.predict(x_test)\n",
    "test_preds = np.round(test_preds)\n",
    "correct_predictions = float(sum(test_preds == y_test)[0])\n",
    "print(\"Correct predictions:\", correct_predictions)\n",
    "print(\"Total number of test examples:\", len(y_test))\n",
    "print(\"Accuracy of model1: \", correct_predictions/float(len(y_test)))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "x_pred = modell.predict(x_test)\n",
    "x_pred = np.round(x_pred)\n",
    "x_pred = x_pred.argmax(1)\n",
    "y_test_s = y_test.argmax(1)\n",
    "cm = confusion_matrix(y_test_s, x_pred)\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(cm,annot=True,annot_kws={\"size\": 16},fmt='1f')\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test_s, x_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ae9d2",
   "metadata": {},
   "source": [
    "## Modeling with English dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c563f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, concatenate, Dropout\n",
    "from keras.models import Model\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf99de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training_English = pd.read_excel(\"drive/My Drive/classes_English.xlsx\")\n",
    "texts = data_training_English[\"full_text\"]\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "labels = []\n",
    "for i in range(data_training_English.text.shape[0]):\n",
    " labels.append(data_training_English.type[i])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels),num_classes = 2)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "x_train, x_test, y_train, y_test = train_test_split( data, labels, test_size=0.20, random_state=42)\n",
    "x_test, x_val, y_test, y_val = train_test_split( x_test, y_test, test_size=0.50, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "%matplotlib inline\n",
    "\n",
    "GLOVE_DIR = \"drive/My Drive/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    " values = line.split()\n",
    " word = values[0]\n",
    " coefs = np.asarray(values[1:], dtype='float32')\n",
    " embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Total %s word vectors in Glove.' % len(embeddings_index))\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    " embedding_vector = embeddings_index.get(word)\n",
    " if embedding_vector is not None:\n",
    " embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    " EMBEDDING_DIM,\n",
    "weights=[embedding_matrix],\n",
    "input_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vecor_length = 32\n",
    "modell = Sequential()\n",
    "modell.add(embedding_layer)\n",
    "modell.add(Dropout(0.2))\n",
    "modell.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'))\n",
    "modell.add(MaxPooling1D(pool_size=2))\n",
    "modell.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "modell.add(MaxPooling1D(pool_size=2))\n",
    "modell.add(LSTM(8, dropout=0.2, recurrent_dropout=0.2))\n",
    "modell.add(BatchNormalization())\n",
    "modell.add(Dense(256, activation='relu'))\n",
    "modell.add(Dense(128, activation='relu'))\n",
    "modell.add(Dense(64, activation='relu'))\n",
    "modell.add(Dense(2, activation='sigmoid'))\n",
    "modell.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(modell.summary())\n",
    "history = modell.fit(x_train, y_train,validation_split=0.2, epochs=30, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b8d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "test_preds = modell.predict(x_test)\n",
    "test_preds = np.round(test_preds)\n",
    "correct_predictions = float(sum(test_preds == y_test)[0])\n",
    "print(\"Correct predictions:\", correct_predictions)\n",
    "print(\"Total number of test examples:\", len(y_test))\n",
    "print(\"Accuracy of model1: \", correct_predictions/float(len(y_test)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "x_pred = modell.predict(x_test)\n",
    "x_pred = np.round(x_pred)\n",
    "x_pred = x_pred.argmax(1)\n",
    "y_test_s = y_test.argmax(1)\n",
    "cm = confusion_matrix(y_test_s, x_pred)\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(cm,annot=True,annot_kws={\"size\": 16},fmt='1f')\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test_s, x_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
